'''
    PerceiverIO code based off of https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_pytorch.py
'''
import warnings
import math
from math import pi, log
from functools import wraps

import torch
from torch import nn, einsum
from torch.nn import functional as F

from einops import rearrange, repeat
from einops.layers.torch import Reduce

from robomimic.models.base_nets import Module

# Helper functions

def exists(val):
    return val is not None

def default(val, d):
    return val if exists(val) else d

def cache_fn(f):
    cache = dict()
    @wraps(f)
    def cached_fn(*args, _cache=True, key=None, **kwargs):
        if not _cache:
            return f(*args, **kwargs)
        nonlocal cache
        if key in cache:
            return cache[key]
        result = f(*args, **kwargs)
        cache[key] = result
        return result
    return cached_fn

# Helper functions/classes for tokenizing data

class RGBPatchEmbed(nn.Module):
    def __init__(
        self,
        img_size=84,
        img_patch_size=14,
        in_channels=3,
        embed_dim=384,
    ):
        super().__init__()
        self.img_patches = int((img_size/img_patch_size)*(img_size/img_patch_size))
        self.img_size = img_size
        self.embed_dim = embed_dim
        self.rgb_proj = nn.Conv2d(in_channels, embed_dim, kernel_size=img_patch_size, stride=img_patch_size)
    
    def forward(self, image):
        # Input shape: batch, sequence, in_channels, H, W
        # Output shape: batch, sequence, patches, embed_dim
        B, S, C, H, W = image.shape
        image = image.view(B * S, C, H, W)
        patched_image = self.rgb_proj(image).flatten(2).transpose(1,2).view(B, S, -1, self.embed_dim)
        return patched_image

class TactilePatchEmbed(nn.Module):
    def __init__(
        self,
        tactile_dim=12,
        num_tactile_patches=32, # Should be the same as desired FT history length
        embed_dim=384,
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.tactile_patches = num_tactile_patches
        self.tactile_proj = nn.Linear(tactile_dim, embed_dim)

    def forward(self, tactile):
        # Input shape: batch, sequence, history length, tactile_dim
        # Output shape: batch, sequence, history length, embed_dim
        B, S, H, N = tactile.shape
        tactile = tactile.reshape((B*S*H, -1))
        patched_tactile = self.tactile_proj(tactile).view(B, S, self.tactile_patches, -1)
        return patched_tactile

# From original VTT paper (only takes in current force-torque reading)
class TactilePatchEmbedNoHistory(nn.Module):
    def __init__(
        self,
        tactile_dim=12,
        num_tactile_patches=4,
        embed_dim=384,
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.tactile_patches = num_tactile_patches
        self.tactile_proj = nn.Linear(tactile_dim, num_tactile_patches*embed_dim)
    
    def forward(self, tactile):
        # Input shape: batch, sequence, tactile_dim
        # Output shape: batch, sequence, num_tactile_patches, embed_dim
        tactile = tactile.squeeze(2)
        B, S, N = tactile.shape
        tactile = tactile.view(B*S, -1)
        patched_tactile = self.tactile_proj(tactile).view(B, S, self.tactile_patches, -1)
        return patched_tactile

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

# Helper classes

class PreNorm(nn.Module):
    def __init__(self, dim, fn, context_dim=None):
        super().__init__()
        self.fn = fn
        self.norm = nn.LayerNorm(dim)
        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None
    
    def forward(self, x, **kwargs):
        x = self.norm(x)

        if exists(self.norm_context):
            context = kwargs['context']
            normed_context = self.norm_context(context)
            kwargs.update(context=normed_context)
        
        return self.fn(x, **kwargs)

class GeGELU(nn.Module):
    def forward(self, x):
        x, gates = x.chunk(2, dim=-1)
        return x * F.gelu(gates)

class FeedForward(nn.Module):
    def __init__(self, dim, mlp_ratio=4.0):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, dim*int(mlp_ratio)*2),
            GeGELU(),
            nn.Linear(dim*int(mlp_ratio), dim),
        )
    
    def forward(self, x):
        return self.net(x)

class Attention(nn.Module):
    def __init__(
        self, 
        dim,
        num_heads=8,
        qkv_bias=False,
        qk_scale=None,
        attn_drop=0.0,
        proj_drop=0.0
    ):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        self.to_q = nn.Linear(dim, dim, bias=qkv_bias)
        self.to_kv = nn.Linear(dim, dim*2, bias=qkv_bias)
        self.proj = nn.Linear(dim, dim)

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj_drop = nn.Dropout(proj_drop)
    
    def forward(self, x, context=None, mask=None, return_attention: bool = False):
        B, S, N, D = x.shape
        N_context = context.shape[2] if exists(context) else N

        h = self.num_heads
        context = default(context, x)

        x = rearrange(x, 'b s n d -> (b s) n d')
        context = rearrange(context, 'b s n d -> (b s) n d')

        q = self.to_q(x)
        k,v = self.to_kv(context).chunk(2, dim=-1)

        q,k,v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q,k,v))
        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale

        if exists(mask):
            mask = rearrange(mask, 'b s ... -> (b s) (...)')
            max_neg_value = -torch.finfo(sim.dtype).max
            mask = repeat(mask, 'b j -> (b h) () j', h=h)
            sim = sim.masked_fill_(~mask, max_neg_value)
        
        attn = sim.softmax(dim=-1)
        attn = self.attn_drop(attn)

        out = einsum('b i j, b j d -> b i d', attn, v)
        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)
        out = rearrange(out, '(b s) n d -> b s n d', s=S)

        out = self.proj(out)
        out = self.proj_drop(out)

        attn = attn.view(B, S, -1, N, N_context)
        if return_attention:
            return attn

        return out

class PerceiverVTT(Module):
    def __init__(
        self,
        img_sizes=(84,),
        img_patch_size=14,
        tactile_dim=12,
        tactile_patches=4,      # Only used for original VTT (no history)
        tactile_history=32,     # Set to 1 for original VTT
        num_latents=64,
        embed_dim=384,
        output_dim=288,
        depth_vtt=3,
        depth_latent=3,
        num_heads=8,
        mlp_ratio=4.0,
        qkv_bias=False,
        qk_scale=None,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        token_drop_rate=0.0,
        **kwargs
    ):
        super().__init__()
        self.output_dim = output_dim
        self.output_dim = output_dim
        self.img_patch_size = img_patch_size
        self.tactile_patches = tactile_patches
        self.tactile_history = tactile_history
        self.token_drop_rate = token_drop_rate
        self.num_rgb = len(img_sizes)

        # Patch embedding networks for separate modalities
        self.rgb_patch_embed = RGBPatchEmbed(
            img_size=img_sizes[0],
            img_patch_size=img_patch_size,
            in_channels=3,
            embed_dim=embed_dim
        )

        self.depth_patch_embed = RGBPatchEmbed(
            img_size=img_sizes[0],
            img_patch_size=img_patch_size,
            in_channels=1,
            embed_dim=embed_dim
        )

        if tactile_history == 1:
            self.tactile_patch_embed = TactilePatchEmbedNoHistory(
                tactile_dim=tactile_dim,
                num_tactile_patches=tactile_patches,
                embed_dim=embed_dim
            )
        else:
            self.tactile_patch_embed = TactilePatchEmbed(
                tactile_dim=tactile_dim,
                num_tactile_patches=tactile_history,
                embed_dim=embed_dim
            )
        
        img_patches = self.rgb_patch_embed.img_patches
        tactile_patches = self.tactile_patch_embed.tactile_patches

        # Set up modality-specific positional embeddings
        self.pos_enc = nn.ParameterDict({
            'rgb': None,
            'depth': None,
            'tactile': None,
        })

        self.pos_enc['rgb'] = nn.Parameter(torch.zeros(1, img_patches, embed_dim))
        self.pos_enc['depth'] = nn.Parameter(torch.zeros(1, img_patches, embed_dim))
        self.pos_enc['tactile'] = nn.Parameter(torch.zeros(1, tactile_patches, embed_dim))

        # Initialize modality/position embedding parameters
        for m in self.pos_enc.keys():
            if exists(self.pos_enc[m]):
                trunc_normal_(self.pos_enc[m])

        self.latents = nn.Parameter(torch.randn(num_latents, embed_dim))

        # Set up attention encoder layers
        self.cross_attend_blocks = nn.ModuleList([
            PreNorm(embed_dim, Attention(
                embed_dim, 
                num_heads=num_heads,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                attn_drop=attn_drop_rate,
                proj_drop=drop_rate
            ), context_dim=embed_dim),
            PreNorm(embed_dim, FeedForward(embed_dim, mlp_ratio=mlp_ratio))
        ])

        get_self_attn = lambda: PreNorm(embed_dim, Attention(
            embed_dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            attn_drop=attn_drop_rate,
            proj_drop=drop_rate
        ))
        get_ff = lambda: PreNorm(embed_dim, FeedForward(embed_dim, mlp_ratio=mlp_ratio))

        get_self_attn, get_ff = map(cache_fn, (get_self_attn, get_ff))

        self.vtt_layers = nn.ModuleList([])
        self.latent_layers = nn.ModuleList([])

        for i in range(depth_vtt):
            self.vtt_layers.append(nn.ModuleList([
                get_self_attn(), get_ff()
            ]))
        
        for i in range(depth_latent):
            self.latent_layers.append(nn.ModuleList([
                get_self_attn(), get_ff()
            ]))
        
        # Set up post-attention layers
        self.norm = nn.LayerNorm(embed_dim)
        self.compress_patches = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 4),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(embed_dim // 4, embed_dim // 12)
        )
        self.compress_layer = nn.Sequential(
            nn.Linear(num_latents * embed_dim // 12, 640),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(640, output_dim)
        )

    # Prepares observation tokens as input to attention layers
    def prepare_tokens(self, images, depths, tactile):
        # if exists(images):
        #     B, S, NC, w, h = images[0].shape

        # Tokenize observations
        tokens = []
        if exists(images):
            images_patched = [self.rgb_patch_embed(img) for img in images]
            image_tokens = torch.cat(images_patched, dim=2)

            if self.training and self.token_drop_rate > 0.0:
                mask = self.compute_token_dropout_mask(image_tokens)
                image_tokens = image_tokens.masked_fill_(~mask, 0.0)
            
            # Shared position embedding across modality 
            image_tokens += rearrange(repeat(self.pos_enc['rgb'], '1 n d -> r n d', r=self.num_rgb), 'r n d -> (r n) d')
            tokens.append(image_tokens)
        
        if exists(depths):
            depths_patched = [self.depth_patch_embed(d) for d in depths]
            depth_tokens = torch.cat(depths_patched, dim=2)

            if self.training and self.token_drop_rate > 0.0:
                mask = self.compute_token_dropout_mask(depth_tokens)
                depth_tokens = depth_tokens.masked_fill_(~mask, 0.0)
            
            # Shared position embedding across modality
            depth_tokens += rearrange(repeat(self.pos_enc['depth'], '1 n d -> r n d', r=self.num_rgb), 'r n d -> (r n) d')
            tokens.append(depth_tokens)
        
        if exists(tactile):
            tactile_patched = self.tactile_patch_embed(tactile)
            tactile_tokens = tactile_patched

            if self.training and self.token_drop_rate > 0.0:
                mask = self.compute_token_dropout_mask(tactile_tokens)
                tactile_tokens = tactile_tokens.masked_fill_(~mask, 0.0)
            
            tactile_tokens += self.pos_enc['tactile']

            tokens.append(tactile_tokens)

        # Group all tokens together
        x = torch.cat(tokens, dim=2)
        return x
    
    def compute_token_dropout_mask(self, x):
        B, S, N, D = x.shape
        drop_prob = torch.rand((B*S, N), device=x.device)
        mask = drop_prob > self.token_drop_rate
        mask = rearrange(mask, '(b s) j -> b s j 1', b=B, s=S)
        return mask
    
    def compute_attention_blocks(self, x):
        B, S, N, D = x.shape

        # Visuotactile self-attention
        for self_attn_vtt, self_ff_vtt in self.vtt_layers:
            x = self_attn_vtt(x) + x
            x = self_ff_vtt(x) + x

        latents = repeat(self.latents, 'n d -> b s n d', b=B, s=S)

        # Cross-attention with latent vectors
        cross_attn, cross_ff = self.cross_attend_blocks

        x = cross_attn(latents, context=x) + latents
        x = cross_ff(x) + x

        # Latent self-attention
        for self_attn_lat, self_ff_lat in self.latent_layers:
            x = self_attn_lat(x) + x
            x = self_ff_lat(x) + x
        
        return x


    def forward(self, images: list, depths: list, tactile: list):
        if len(images) > 0:
            if images[0].ndim != 5:
                images = [img.unsqueeze(1) for img in images]
            assert images[0].ndim == 5
        else:
            images = None
        
        if len(depths) > 0:
            if depths[0].ndim != 5:
                depths = [d.unsqueeze(1) for d in depths]
            assert depths[0].ndim == 5
        else:
            depths = None
        
        if len(tactile) > 0:
            tactile = tactile[0][..., -self.tactile_history:, :]
            if tactile.ndim != 4:
                tactile = tactile.unsqueeze(1)
            assert tactile.ndim == 4
        else:
            tactile = None

        # Tokenize observations and add position encodings
        x = self.prepare_tokens(images, depths, tactile)

        # Feed tokens through attention layers
        x = self.compute_attention_blocks(x)

        # Normalize attention outputs
        x = self.norm(x)

        # Compress patches before flattening and concatenation
        x_out = self.compress_patches(x)

        # Flatten and transform output tokens into single latent vector
        B, S, patches, dim = x_out.size()
        x_out = x_out.view(B, S, -1)
        z = self.compress_layer(x_out).squeeze(1)

        return z
    
    # =============== For visualization purposes
    # TODO: Get latent cross-attention layer attentions
    def get_last_selfattn(self, images, depths, tactile):
        if len(images) > 0:
            images = [img.unsqueeze(1) for img in images]
        else:
            images = None
        
        if len(depths) > 0:
            depths = [d.unsqueeze(1) for d in depths]
        else:
            depths = None
        
        if len(tactile) > 0:
            tactile =  tactile[..., -self.tactile_history:].unsqueeze(1)
        else:
            tactile = None
        
        x = self.prepare_tokens(images, depths, tactile)
        B, S, N, D = x.shape

        # Visuotactile self-attention
        for self_attn_vtt, self_ff_vtt in self.vtt_layers:
            x = self_attn_vtt(x) + x
            x = self_ff_vtt(x) + x
        
        latents = repeat(self.latents, 'n d -> b s n d', b=B, s=S)

        # Cross-attention with latent vectors
        cross_attn, cross_ff = self.cross_attend_blocks
        attn = cross_attn(latents, context=x, return_attention=True)
        return attn
    
    def visualize_attention(self, images, tactile):
        # make the images divisible by the patch size
        w = images[0].shape[-1] - images[0].shape[-1] % self.img_patch_size
        h = images[0].shape[-2] - images[0].shape[-2] % self.img_patch_size
        images = [img[...,:h,:w] for img in images]

        w_featmap = images[0].shape[-1] // self.img_patch_size
        h_featmap = images[0].shape[-2] // self.img_patch_size

        attentions = self.get_last_selfattn(images=images, depths=[], tactile=tactile).squeeze(1)
        nh = attentions.shape[1] # number of heads

        # keep only the output patch attention
        # TODO: aggregate attentions across latents?
        # attentions = attentions[0, :, 0, :]
        attentions = torch.mean(attentions[0], dim=1)

        # average the attentions across all heads
        attentions = torch.mean(attentions, dim=0)

        # separate modality-specific attentions
        img_patches = self.rgb_patch_embed.img_patches
        tactile_patches = self.tactile_patch_embed.tactile_patches
        img_attns = [attentions[i*img_patches:(i+1)*img_patches] for i in range(self.num_rgb)]
        tactile_attns = attentions[-tactile_patches:]

        # calculate modality-specific attention proportions
        img_proportion = torch.sum(attentions[:-tactile_patches]) / torch.sum(attentions)
        tactile_proportion = torch.sum(tactile_attns) / torch.sum(attentions)
        proportions = (img_proportion, tactile_proportion)

        # calculating attention proportions within modality-specific attention
        img_attns = [img_attn / torch.sum(img_attn) for img_attn in img_attns]
        tactile_attns = tactile_attns / torch.sum(tactile_attns)

        # turning image attentions into per-image heatmaps
        img_attns = [
            F.interpolate(
                img_attn.reshape(1, h_featmap, w_featmap).unsqueeze(0),
                scale_factor=self.img_patch_size,
                mode="bilinear"
            )[0] for img_attn in img_attns
        ]

        return img_attns, tactile_attns, proportions

    # ==========================================
    
    def output_shape(self, input_shape=None):
        return [self.output_dim]

class PerceiverViT(Module):
    def __init__(
        self,
        img_sizes=(84,),
        img_patch_size=14,
        num_latents=64,
        embed_dim=384,
        output_dim=288,
        depth_vtt=3,
        depth_latent=3,
        num_heads=8,
        mlp_ratio=4.0,
        qkv_bias=False,
        qk_scale=None,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        token_drop_rate=0.0,
        **kwargs
    ):
        super().__init__()
        self.output_dim = output_dim
        self.output_dim = output_dim
        self.img_patch_size = img_patch_size
        self.token_drop_rate = token_drop_rate

        # Patch embedding networks for separate modalities
        self.rgb_patch_embed = RGBPatchEmbed(
            img_size=img_sizes[0],
            img_patch_size=img_patch_size,
            in_channels=3,
            embed_dim=embed_dim
        )

        self.depth_patch_embed = RGBPatchEmbed(
            img_size=img_sizes[0],
            img_patch_size=img_patch_size,
            in_channels=1,
            embed_dim=embed_dim
        )
        
        img_patches = self.rgb_patch_embed.img_patches

        # Set up modality-specific positional embeddings
        self.pos_enc = nn.ParameterDict({
            'rgb': None,
            'depth': None,
        })

        self.pos_enc['rgb'] = nn.Parameter(torch.zeros(1, img_patches, embed_dim))
        self.pos_enc['depth'] = nn.Parameter(torch.zeros(1, img_patches, embed_dim))

        # Initialize modality/position embedding parameters
        for m in self.pos_enc.keys():
            if exists(self.pos_enc[m]):
                trunc_normal_(self.pos_enc[m])

        self.latents = nn.Parameter(torch.randn(num_latents, embed_dim))

        # Set up attention encoder layers
        self.cross_attend_blocks = nn.ModuleList([
            PreNorm(embed_dim, Attention(
                embed_dim, 
                num_heads=num_heads,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                attn_drop=attn_drop_rate,
                proj_drop=drop_rate
            ), context_dim=embed_dim),
            PreNorm(embed_dim, FeedForward(embed_dim, mlp_ratio=mlp_ratio))
        ])

        get_self_attn = lambda: PreNorm(embed_dim, Attention(
            embed_dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            attn_drop=attn_drop_rate,
            proj_drop=drop_rate
        ))
        get_ff = lambda: PreNorm(embed_dim, FeedForward(embed_dim, mlp_ratio=mlp_ratio))

        get_self_attn, get_ff = map(cache_fn, (get_self_attn, get_ff))

        self.vtt_layers = nn.ModuleList([])
        self.latent_layers = nn.ModuleList([])

        for i in range(depth_vtt):
            self.vtt_layers.append(nn.ModuleList([
                get_self_attn(), get_ff()
            ]))
        
        for i in range(depth_latent):
            self.latent_layers.append(nn.ModuleList([
                get_self_attn(), get_ff()
            ]))
        
        # Set up post-attention layers
        self.norm = nn.LayerNorm(embed_dim)
        self.compress_patches = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 4),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(embed_dim // 4, embed_dim // 12)
        )
        self.compress_layer = nn.Sequential(
            nn.Linear(num_latents * embed_dim // 12, 640),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(640, output_dim)
        )

    # Prepares observation tokens as input to attention layers
    def prepare_tokens(self, images, depths):

        # Tokenize observations
        tokens = []
        if exists(images):
            images_patched = [self.rgb_patch_embed(img) for img in images]
            image_tokens = torch.cat(images_patched, dim=2)

            if self.training and self.token_drop_rate > 0.0:
                mask = self.compute_token_dropout_mask(image_tokens)
                image_tokens = image_tokens.masked_fill_(~mask, 0.0)
            
            image_tokens += self.pos_enc['rgb']
            tokens.append(image_tokens)
        
        if exists(depths):
            depths_patched = [self.depth_patch_embed(d) for d in depths]
            depth_tokens = torch.cat(depths_patched, dim=2)

            if self.training and self.token_drop_rate > 0.0:
                mask = self.compute_token_dropout_mask(depth_tokens)
                depth_tokens = depth_tokens.masked_fill_(~mask, 0.0)
            
            depth_tokens += self.pos_enc['depth']
            tokens.append(depth_tokens)

        # Group all tokens together
        x = torch.cat(tokens, dim=2)
        return x
    
    def compute_token_dropout_mask(self, x):
        B, S, N, D = x.shape
        drop_prob = torch.rand((B*S, N), device=x.device)
        mask = drop_prob > self.token_drop_rate
        mask = rearrange(mask, '(b s) j -> b s j 1', b=B, s=S)
        return mask
    
    def compute_attention_blocks(self, x):
        B, S, N, D = x.shape

        # Visuotactile self-attention
        for self_attn_vtt, self_ff_vtt in self.vtt_layers:
            x = self_attn_vtt(x) + x
            x = self_ff_vtt(x) + x

        latents = repeat(self.latents, 'n d -> b s n d', b=B, s=S)

        # Cross-attention with latent vectors
        cross_attn, cross_ff = self.cross_attend_blocks

        x = cross_attn(latents, context=x) + latents
        x = cross_ff(x) + x

        # Latent self-attention
        for self_attn_lat, self_ff_lat in self.latent_layers:
            x = self_attn_lat(x) + x
            x = self_ff_lat(x) + x
        
        return x


    def forward(self, images: list, depths: list, tactile: list):
        if len(images) > 0:
            if images[0].ndim != 5:
                images = [img.unsqueeze(1) for img in images]
            assert images[0].ndim == 5
        else:
            images = None
        
        if len(depths) > 0:
            if depths[0].ndim != 5:
                depths = [d.unsqueeze(1) for d in depths]
            assert depths[0].ndim == 5
        else:
            depths = None

        # Tokenize observations and add position encodings
        x = self.prepare_tokens(images, depths)

        # Feed tokens through attention layers
        x = self.compute_attention_blocks(x)

        # Normalize attention outputs
        x = self.norm(x)

        # Compress patches before flattening and concatenation
        x_out = self.compress_patches(x)

        # Flatten and transform output tokens into single latent vector
        B, S, patches, dim = x_out.size()
        x_out = x_out.view(B, S, -1)
        z = self.compress_layer(x_out).squeeze(1)

        return z
    
    # =============== For visualization purposes
    # TODO: Get latent cross-attention layer attentions
    def get_last_selfattn(self, images, depths, tactile):
        raise NotImplementedError
    
    def visualize_attention(self, images, tactile):
        # TODO
        raise NotImplementedError 
    
    def output_shape(self, input_shape=None):
        return [self.output_dim]